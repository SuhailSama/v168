---
title: Sample-based Distributional Policy Gradient
abstract: Distributional reinforcement learning (DRL) is a recent reinforcement learning
  framework whose success has been supported by various empirical studies. It relies
  on the idea of replacing the expected return with the return distribution, which
  captures the intrinsic randomness of the long term rewards. Most of the existing
  literature on DRL focuses on problems with discrete action space and value based
  methods. In this work, motivated by applications in control engineering and robotics
  where the action space is continuous, we propose the sample-based distributional
  policy gradient (SDPG) algorithm. It models the return distribution using samples
  via a reparameterization technique widely used in generative modeling. We compare
  SDPG with the state-of-the-art policy gradient method in DRL, distributed distributional
  deterministic policy gradients (D4PG). We apply SDPG and D4PG to multiple OpenAI
  Gym environments and observe that our algorithm shows better sample efficiency as
  well as higher reward for most tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: singh22a
month: 0
tex_title: Sample-based Distributional Policy Gradient
firstpage: 676
lastpage: 688
page: 676-688
order: 676
cycles: false
bibtex_author: Singh, Rahul and Lee, Keuntaek and Chen, Yongxin
author:
- given: Rahul
  family: Singh
- given: Keuntaek
  family: Lee
- given: Yongxin
  family: Chen
date: 2022-05-11
address:
container-title: Proceedings of The 4th Annual Learning for Dynamics and Control Conference
volume: '168'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 11
pdf: https://proceedings.mlr.press/v168/singh22a/singh22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
