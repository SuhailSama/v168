---
title: Improving Dynamic Regret in Distributed Online Mirror Descent Using Primal
  and Dual Information
abstract: We consider the problem of distributed online optimization, with a group
  of learners connected via a dynamic communication graph. The goal of the learners
  is to track the global minimizer of a sum of time-varying loss functions in a distributed
  manner. We propose a novel algorithm, termed Distributed Online Mirror Descent with
  Multiple Averaging Decision and Gradient Consensus (DOMD-MADGC), which is based
  on mirror descent but incorporates multiple consensus averaging iterations over
  local gradients as well as local decisions. The key idea is to allow the local learners
  to collect a sufficient amount of global information, which enables them to more
  accurately approximation the time-varying global loss, so that they can closely
  track the dynamic global minimizer over time. We show that the dynamic regret of
  DOMD-MADGC is upper bounded by the path length, which is defined as the cumulative
  distance between successive minimizers. The resulting bound improves upon the bounds
  of existing distributed online algorithms and removes the explicit dependence on
  $T$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eshraghi22a
month: 0
tex_title: Improving Dynamic Regret in Distributed Online Mirror Descent Using Primal
  and Dual Information
firstpage: 637
lastpage: 649
page: 637-649
order: 637
cycles: false
bibtex_author: Eshraghi, Nima and Liang, Ben
author:
- given: Nima
  family: Eshraghi
- given: Ben
  family: Liang
date: 2022-05-11
address:
container-title: Proceedings of The 4th Annual Learning for Dynamics and Control Conference
volume: '168'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 5
  - 11
pdf: https://proceedings.mlr.press/v168/eshraghi22a/eshraghi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
